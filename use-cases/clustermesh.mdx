---
description: A guide for using Calico's cluster mesh.
title: Cluster mesh
---

# Cluster mesh

This guide explains the concept of cluster mesh and how you can use Calico to enable cross-cluster communications, service discovery, and federated endpoints.

## Overview

### What is cluster mesh?

Cluster mesh enables cross-cluster communication in a Kubernetes-native way.

Pods and workloads in one cluster can easily access and use services and endpoints from another cluster as if they’re local, meaning you can access them with their local DNS records and use `kubectl` to interact with them.
These are known as federated services or endpoints

### When to use cluster mesh

If you already have, or are planning an environment with multiple clusters and you
* Want to share, or federate services between clusters
* Use network policies to manage access to federated services and endpoints to meet compliance requirements
* Need high availability (HA), disaster recovery (DR) or low latency for your services
* Don’t want to set up or maintain a service mesh


#### Federate or share services or endpoints between clusters

Federated services in Kubernetes allow you to access and discover services from other clusters as if they exist in your local cluster.
This provides a Kubernetes-native experience because you can connect to services and reference them through their annotations and service names.

#### Secure federated services and endpoints for compliance

When services are federated, you can apply network policies to the shared services as if they are local to the cluster using labels and selectors, rather than IP addresses.
This aids in achieving compliance by ensuring you have the required security access controls in place to protect inter-cluster services.
The [egress access control](https://docs.tigera.io/use-cases/egress-access-controls) use case expands on this topic.

#### Improve networking performance

Federated services provide a low-latency solution because traffic travels directly to the service in the remote cluster using an overlay network, without the need for an additional hop through ingress controllers, load balancers, or other routes.
This is beneficial for low-latency applications that are distributed or communicate across different clusters.
Calico Enterprise cluster mesh is implemented at Kubernetes at the network layer, based on pod IPs.

#### Reduce downtime

High availability means your service should be available continuously with no, or minimal, downtime.
Federated services can be backed by geographically distributed clusters in different availability zones, regions, or clouds.
This ensures that if one of the services goes down, there are others that are still up and running that can respond to requests without intervention.
A workload accessing the federated service will not be interrupted.

#### Recover faster

Federated services are auto-discovered by clusters meshed together.
In the case of a disaster, as soon as the recovery or backup service is up and running with the correct namespace and annotations, clients will be able to access it.
With disaster recovery, some service interruption is expected, but federating services with cluster mesh means that clients can resume connections faster with no manual re-configurations.

#### Avoid using service mesh

A Kubernetes service mesh is a dedicated infrastructure layer designed to manage, observe, and control communication between microservices within a Kubernetes cluster, with the goal to improve the overall reliability, security, and observability of the microservices that make up a complex, distributed application.
Oftentimes, installing and managing a service mesh requires specialist expertise or is considered not to be worth the time and effort invested for a small subset of service mesh functionality.
Using an overlay network to achieve cross-cluster communication can be a faster, simpler solution without the overhead of a service mesh.

## Support for cluster mesh

Cluster mesh is supported in Calico Enterprise or Calico Cloud out-of-box.

You can watch this [CNCF webinar recording](https://www.cncf.io/online-programs/cncf-on-demand-webinar-kubernetes-cluster-mesh-with-open-source-calico/), which shows BGP routing, peering, and Calico BIRD integration in the cloud with Calico Open Source.

To see what’s possible with all Calico products, you can read this blog post “[What is a Kubernetes cluster mesh and what are the benefits?](https://www.tigera.io/blog/deep-dive/what-is-a-kubernetes-cluster-mesh-and-what-are-the-benefits/)” 

## Cluster mesh concepts

Cluster mesh can be an advanced topic, so we recommend you understand the different components and configurations before implementing this in your own environment.

Before getting into the concepts, this is what we mean by:
* Federated endpoint identity - allow a local Kubernetes cluster to include the workload endpoints (pods) and host endpoints of a remote cluster in the calculation of local network policies applied on each node of the local cluster.
* Federated services - enable a local Kubernetes Service to populate with Endpoints selected from both local cluster and remote cluster Services.
* Multi-cluster networking - establish an overlay network between clusters to provide cross-cluster connectivity with Calico Enterprise.

### Cluster mesh networking

Calico Enterprise or Calico Cloud cluster mesh is implemented in Kubernetes at the network layer, based on pod IPs (layer 3 of the OSI model).
There are two options for implementing this: overlay, or underlay networking.

An overlay network creates a virtual network on top of a physical network, allowing pods to communicate directly with each other.
This is beneficial when the underlying network cannot easily be made aware of other workload IPs, for example, across multiple VPCs or subnets.
For cluster mesh, packets use VXLAN encapsulation.

Calico Enterprise or Calico Cloud can automatically extend the VXLAN overlay when `RemoteClusterConfigurations` are created.

With an underlay network, such as BGP, routing information between nodes is shared between peers.
This allows peers, or nodes, to ensure their workloads are communicable, because pod CIDRs are advertised to the underlying network and traffic can be routed to them successfully.
This means pods can communicate using their IP addresses, but Kubernetes nomenclature for targeting or selecting endpoints.

Calico cluster mesh offers both BGP (underlay) and overlay, and they can be used in conjunction with each other.
Configuring the underlay networking ensures that services and workloads are routable between nodes, and the overlay enables the Kubernetes-native approach of communicating.

### Cluster types

When configuring cluster mesh you might see a reference to a local or remote cluster.
Each cluster in the cluster mesh can act as both a local and a remote cluster.
Local clusters are configured to retrieve endpoint and routing data from remote clusters (via RemoteClusterConfiguration).
Remote clusters authorize local clusters to retrieve endpoint and routing data.

### RemoteClusterConfiguration

RemoteClusterConfiguration is the resource that configures a local cluster to sync resources from a remote cluster.
It primarily describes how a local cluster establishes that connection to the remote cluster through which resources are synced.
The resources synced through this one-way connection enable the local cluster to reference a remote endpoint identity and establish cross-cluster overlay routes.
If the remote cluster needs to sync resources that exist in the local cluster, it will need its own RemoteClusterConfiguration.
In this case, the remote cluster becomes the local cluster, and vice versa.

### Typha

Typha is a component of Calico that caches the datastore state and maintains a single datastore connection for all its clients.
Typha is the component that will be accessing the remote cluster.
Typha logs will contain the remote cluster connection status.


## Implementing cluster mesh

In order to set up cluster mesh with Calico Enterprise or Calico Cloud, you will need to make sure that you are using Calico CNI, and that you have [ensured pod IP routability](https://docs.tigera.io/calico-enterprise/latest/multicluster/federation/kubeconfig#ensure-pod-ip-routability).

An overview of the key steps to set up cluster mesh are outlined below.

:::note
This is not a prescriptive list, and it doesn’t include any specific networking prerequisites that will vary depending on your environment. Please refer to our detailed instructions that are available in either the Calico Enterprise or Calico Cloud documentation.
:::

### Set up multi-cluster communication

Multi-cluster communication is critical so that pods are routable between clusters.
The first step is to create a dedicated kubeconfig file for each cluster in the mesh that allows for connection and authentication with each other.

Tigera provides manifests that can be applied to create a ServiceAccount for remote clusters to authenticate, and ClusterRole and ClusterRoleBindings if RBAC is enabled.

The kubeconfig file will look something like this:

```yaml
apiVersion: v1
kind: Config
users:
  - name: tigera-federation-remote-cluster
    user:
      token: <YOUR-SERVICE-ACCOUNT-TOKEN>
clusters:
  - name: tigera-federation-remote-cluster
    cluster:
      certificate-authority-data: <YOUR-CERTIFICATE-AUTHORITY-DATA>
      server: <YOUR-SERVER-ADDRESS>
contexts:
  - name: tigera-federation-remote-cluster-ctx
    context:
      cluster: tigera-federation-remote-cluster
      user: tigera-federation-remote-cluster
current-context: tigera-federation-remote-cluster-ctx
```

For detailed instructions on correctly creating these kubeconfig files, follow the documentation for either [Calico Enterprise](https://docs.tigera.io/calico-enterprise/latest/multicluster/federation/kubeconfig#create-kubeconfig-files) or [Calico Cloud](https://docs.tigera.io/calico-cloud/multicluster/kubeconfig#create-kubeconfig-files).

### RemoteClusterConfigurations

RemoteClusterConfigurations enable synchronization between clusters by creating the correct roles, role bindings, and secrets that enable a local cluster to access a remote cluster.

The RemoteClusterConfiguration example shown below specifies the secret used to access cluster B, and the overlay routing mode which toggles the establishment of cross-cluster overlay routes.

```yaml
apiVersion: projectcalico.org/v3
kind: RemoteClusterConfiguration
metadata:
 name: cluster-b
spec:
 clusterAccessSecret:
   name: remote-cluster-secret-name
   namespace: <secret-namespace>
   kind: Secret
 syncOptions:
   overlayRoutingMode: Enabled
```

If you choose not to use an overlay network with VXLAN encapsulation and instead configure it manually, this will be disabled in the RemoteClusterConfiguration:

```yaml
 syncOptions:
   overlayRoutingMode: Disabled
```

If you are setting up an unencapsulated cluster mesh and you have IP pools in cluster A with NAT-outgoing enabled, and workloads in that pool will egress to workloads in cluster B, you need to instruct Calico Enterprise or Calico Cloud to not perform NAT on traffic destined for IP pools in cluster B.

You can achieve this by creating a disabled IP pool in cluster A for each CIDR in cluster B. This IP pool should have NAT-outgoing disabled.
For example:

```yaml
apiVersion: projectcalico.org/v3
kind: IPPool
metadata:
  name: clusterB-main-pool
spec:
  cidr: <Cluster B CIDR>
  disabled: true
```

You can check remote cluster connection status inside the typha logs:

Run the following command:
```bash
kubectl logs deployment/calico-typha -n calico-system | grep "Sending in-sync update"
```

You should see an entry for each RemoteClusterConfiguration in the local cluster.

These steps will need to be performed on both the local and remote clusters to allow bi-directional synchronization.

For detailed instructions on correctly creating the RemoteClusterConfiguration, follow the documentation for either [Calico Enterprise](https://docs.tigera.io/calico-enterprise/latest/multicluster/federation/kubeconfig#create-remoteclusterconfigurations) or [Calico Cloud](https://docs.tigera.io/calico-cloud/multicluster/kubeconfig#create-remoteclusterconfigurations).


### Enable multi-cluster service discovery and federation

The Tigera controller will automatically create and manage federated endpoints once multi-cluster communication is configured.

These endpoints across the mesh can be consolidated into a federated service.

A federated service looks similar to a regular Kubernetes service, but instead of using a pod selector, it uses an annotation to specify the services to federate.
For example:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-app-federated
  namespace: default
  annotations:
    federation.tigera.io/serviceSelector: run == "my-app"
spec:
  ports:
    - name: my-app-ui
      port: 8080
      protocol: TCP
  type: ClusterIP
  ```

Only services in the same namespace as the federated service are included.

For detailed instructions on correctly creating the federated services, follow the documentation for either [Calico Enterprise](https://docs.tigera.io/calico-enterprise/latest/multicluster/federation/services-controller) or [Calico Cloud](https://docs.tigera.io/calico-enterprise/latest/multicluster/federation/services-controller).


### Viewing inter-cluster services

Once you’ve configured federated services you can validate the services and endpoints by running `kubectl get services` and `kubectl get endpoints` for the relevant namespaces in both local and remote clusters.

For a more visual representation, Calico Enterprise and Calico Cloud provide unified observability, allowing you to see federated services as if they’re in your local cluster.
You can read more about [observability](https://docs.tigera.io/use-cases/observability) in our use case page.

You can use the dynamic service and threat graph to see all service resources behind a federated service, and which other endpoints they are interacting with.

![Federated services in service graph](/img/use-cases/federated-services-servicegraph.png)

The flows table auto-filters based on the view inside service graph and you can easily determine which services behind a federated service are being used for each flow.

If you need to identify issues with federated services, having a holistic view of all services (federated and local) within a cluster speeds up that process.

You can see how the bookinfo application (that has been used in examples throughout this use case) is represented in Calico by watching this video: [Observability for Federated Services](https://tigera.wistia.com/medias/qhwbjgh7qi).

## Using cluster mesh

### Using federated services

In the example below, two clusters `uc-aks-prod-01` and `uc-aks-prod-03` have been meshed together, as cluster `uc-aks-prod-03` contains a `reviews` service that the application needs.

The application has two federated services: `reviews` and `ratings`.

The reviews service has been federated in cluster `uc-aks-prod-01`, and there are 3 services backing the federated service which we can see in the diagram.
`Reviews-v1` exists in `uc-akd-prod-01`, and `Reviews-v2` and `Reviews-v2` exist in cluster `uc-aks-prod-03`.

The federated service manifest is looking for any services in the `bookinfo` namespace with the label `app: reviews` using the annotation `federation.tigera.io/serviceSelector: app == "reviews"`.

![bookinfo reference diagram](/img/use-cases/bookinfofederatedservices.png)

In `uc-aks-prod-01`, running a `kubectl get services` will show both the federated `reviews` and `ratings` services, as well as the backing services.
In `uc-aks-prod-03`, running a `kubectl get services` will show the federated `ratings` service, as well as the backing service `reviews-23`.

Running a `kubectl get endpoints` in `uc-aks-prod-01` will show the federated `reviews` service with 3 endpoints (IP addresses of `Reviews-v1`, `Reviews-v2`, and `Reviews-v3`) and the federated `ratings` services with one endpoint (the local `ratings-1` service).

Running a `kubectl get endpoints` in `uc-aks-prod-03` will show the `reviews-23` service with 2 endpoints (IP addresses of `Reviews-v2` and `Reviews-v3`) and the federated `ratings` services with one endpoint (the remote `ratings-1` service).

You may prefer to watch the above examples for [federated endpoints](https://tigera.wistia.com/medias/t5znwissjs) or [federated services](https://tigera.wistia.com/medias/s9a06bebnj) in video format, taking you through the configuration and validation process.

### Achieve High Availability (HA) and Disaster Recovery (DR)

To ensure that you’re set up for high availability, ensure that remote clusters configured on geographically distributed infrastructure to ensure that if one availability zone, region, or cloud experiences an outage, other backing services will not be affected, and the federated service will remain operational.

For disaster recovery, ensure that the recovery components are configured correctly so that when they are brought online, they will be discovered and the endpoints will be available for federated services.
You will need to ensure that ‌multi-cluster communication is pre-configured, along with services having the correct labels in the correct namespaces.


Once the backup clusters and services are running the federated service will continue to work as expected, and the service consumer may have no idea that the endpoints backing the federated service were replaced (apart from an interruption in service) and no additional re-configuration is needed.

Cluster mesh is proven to work in a multi-region Cockroach DB deployment, where database clients need low latency, and database administrators need to enforce data protection controls to restrict access and meet regulatory requirements.
The blog post “[Build and secure multi-cluster CockroachDB using the Calico clustermesh: A step-by-step guide](https://www.tigera.io/blog/build-and-secure-multi-cluster-cockroachdb-using-the-calico-clustermesh-a-step-by-step-guide/)” contains a step-by-step guide to replicate a multi-region CockroachDB deployment with endpoint federation.

Another example, [using Redis](https://www.tigera.io/blog/achieving-high-availabilityha-redis-kubernetes-clusters-with-calico-clustermesh-in-microsoft-aks/), shows how cluster mesh can federate services across multiple clusters to ensure continuous uptime.
[Redis](https://redis.io/docs/latest/operate/kubernetes/architecture/), just like [CockroachDB](https://www.cockroachlabs.com/docs/stable/orchestrate-cockroachdb-with-kubernetes-multi-cluster), has been architectured specifically for Kubernetes environments.
By federating an active-active Redis, this means any services that rely on Redis will be able to access any active instances through the federated service, even if the Redis service in their local cluster were to become unavailable.
You can read more, and see instructions to set up the environment for yourself, in our blog post “Achieving high availability (HA) Redis Kubernetes clusters with Calico Clustermesh in Microsoft AKS”

Without cluster mesh you would need to create and manage a separate ingress or load balancer solution to achieve something similar, but you would lose the ability to target services using their DNS names or labels/annotations.
This would not be desirable for applications that require low latency, as there would be an additional hop as the traffic is routed through an ingress or load balancer.

### Meet compliance requirements and secure federated services

One of the key requirements for most compliance or regulatory frameworks is to implement workload access controls and this extends to federated services.
Using cluster mesh with an overlay network allows network policy to be written and applied in the local cluster containing rules that reference federated services or endpoints using identity-aware labels and selectors.
Without an overlay network or cluster mesh, policies have to be written targeting pod IP addresses, which is not a best practice given that pod IP addresses are often dynamic, which will lead to service interruptions or security risks if workloads within clusters are not adequately protected.

This is discussed in detail in the [microsegmentation](https://docs.tigera.io/use-cases/microsegmentation) and [egress access controls](https://docs.tigera.io/use-cases/egress-access-controls) use case pages, as well as our [resources on compliance](https://www.tigera.io/resources/?_sft_Resource_Topics=compliance).

When applying network security policies in a cluster there is no distinction between local or remote services or endpoints, and will apply as long as the selector correctly matches an endpoint.
Network policies should be applied in both remote and local clusters for comprehensive endpoint coverage.

In the following example, cluster A (an application cluster) has a network policy that governs outbound connections to cluster B (a database cluster).

```yaml
apiversion: projectcalico.org/v3
kind: NetworkPolicy
metadata:
    name: default.app-to-db
    namespace: myapp
spec:
    # The main policy selector selects endpoints from the local cluster only.
    selector: app == 'backend-app'
    tier: default
    egress:
    - destination:
        # Rule selectors can select endpoints from local AND remote clusters.
        selector: app == 'postgres'
      protocol: TCP
      ports: [5432]
      action: Allow
```

You can explore an example of securing and isolating federated endpoints in [this walkthrough](https://github.com/tigera-cs/calico-cloud-unified-control/blob/main/modules/federatedendpoints-1.md).
